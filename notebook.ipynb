{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPTokenizer, CLIPImageProcessor, AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_encoder = clip_model.vision_model\n",
    "text_encoder = clip_model.text_model\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: best match is 'a photo of a cat' (score: 30.380)\n",
      "dog: best match is 'a photo of a dog' (score: 35.998)\n",
      "horse: best match is 'a photo of a horse' (score: 36.964)\n",
      "bird: best match is 'a photo of a bird' (score: 30.397)\n",
      "car: best match is 'a photo of a car' (score: 23.982)\n",
      "person: best match is 'a photo of a person' (score: 27.901)\n",
      "tree: best match is 'a photo of a tree' (score: 30.656)\n",
      "house: best match is 'a photo of a house' (score: 29.412)\n",
      "book: best match is 'a photo of a book' (score: 40.067)\n",
      "phone: best match is 'a photo of a phone' (score: 32.127)\n"
     ]
    }
   ],
   "source": [
    "items = [\"cat\", \"dog\", \"horse\", \"bird\", \"car\", \"person\", \"tree\", \"house\", \"book\", \"phone\"]\n",
    "classifiers = [f\"a photo of a {item}\" for item in items]\n",
    "\n",
    "text_inputs = tokenizer(classifiers, return_tensors=\"pt\")\n",
    "text_features = text_encoder(**text_inputs).pooler_output\n",
    "text_features_proj = clip_model.text_projection(text_features)\n",
    "\n",
    "for item, classifier in zip(items, classifiers):\n",
    "    image = Image.open(f\"images/{item}.jpg\")\n",
    "    image_inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    image_features = vision_encoder(**image_inputs).pooler_output\n",
    "    image_features_proj = clip_model.visual_projection(image_features)\n",
    "    \n",
    "    # Compute similarity to all classifier texts\n",
    "    similarities = (image_features_proj @ text_features_proj.T).squeeze(0)\n",
    "    best_idx = similarities.argmax().item()\n",
    "    best_label = classifiers[best_idx]\n",
    "    best_score = similarities[best_idx].item()\n",
    "    \n",
    "    print(f\"{item}: best match is '{best_label}' (score: {best_score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 乐观 Charter Candle Rescue作文住景供想象 Kiss charter试验今天的景 charter赎回收景acc1追赶景收景景景景景收募\n",
      "dog: ii️À########\n",
      "^)|;;ii️NN}}}️ennyII^^II|enny;;\\tBBBB}}}iii×????ennyaaaaaaaa|\n",
      "BBBB\n",
      "horse: 磨 implied追赶.Unicode.Parcel.Parcelchargesaces派par.Parcel cualquier苗派 Paramountaces Alma Alma派 nar narкультур   charges nar captures派 nar追赶追赶\n",
      "bird: 7完整 E base¡ |完全NC.subtract Letters住完整的 favorable |完整 Waveadera完全 Charterеспدد pledge Rدد¡دد7دد LIVENC\n",
      "car: 绣_int绣夏夏rom只绣 rom绣为主绣owniorown绣夏绣ottie彩绣_rgb_rom高清高清为主rom只loat_pr\n",
      "person: bohydrbohydrbohydrbohydrbohydrbohydrbohydrbohydrbohydrientbohydr่ายbohydrbohydrbohydrbohydrbohydr heatbohydrbohydrbohydrbohydrbohydrbohydrbohydrbohydrbohydr่ายbohydrbohydr\n",
      "tree: uuoo^^¨oooaaaaÿÄeeeeoooooaaÄ^^??mm❤eeeeXXeeehhoooaaaaabcdooxxxxxxuuaaaa||\n",
      "house: ện Labourар قناufficient قناufficientacious Labourufficient Labour...\n",
      "iverse lợiiverseiverseufficient่ายufficientufficientufficientufficientacious labourLabour Labour Labourufficient laborufficient\n",
      "book: 陪汗水微陪ート陪 armour陪�ooled armour汗水微耐�兼酝陪�ugg伴揉揉微ート勇微微aped büyü\n",
      "phone: levationlevation custom ornionalenames decorative decor� ornlevationroup decoratorional ornlevationiftlevationlevationlevationlevation orn족 decor décorunded_redirected decorroupional\n"
     ]
    }
   ],
   "source": [
    "clip_dim = clip_model.visual_projection.out_features  # 512\n",
    "qwen_dim = qwen_model.model.embed_tokens.embedding_dim  # 4096\n",
    "\n",
    "adapter = nn.Sequential(\n",
    "    nn.Linear(clip_dim, qwen_dim),\n",
    "    nn.LayerNorm(qwen_dim),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(qwen_dim, qwen_dim),\n",
    ")\n",
    "\n",
    "for item in items:\n",
    "    image = Image.open(f\"images/{item}.jpg\")\n",
    "    image_inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    image_features = vision_encoder(**image_inputs).pooler_output\n",
    "    image_features_proj = clip_model.visual_projection(image_features)\n",
    "    image_latent = adapter(image_features_proj)\n",
    "    attention_mask = torch.ones(1, 1)\n",
    "    position_ids = torch.zeros(1, 1)\n",
    "\n",
    "    generated_ids = qwen_model.generate(inputs_embeds=image_latent.unsqueeze(1), attention_mask=attention_mask, position_ids=position_ids, max_new_tokens=30, do_sample=True, temperature=0.8, pad_token_id=qwen_tokenizer.pad_token_id, eos_token_id=qwen_tokenizer.eos_token_id)\n",
    "\n",
    "    print(f\"{item}: {qwen_tokenizer.decode(generated_ids[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Flickr8k images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136160it [00:09, 14316.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Flickr8k captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "286it [00:00, 2124.61it/s]\n",
      "100%|██████████| 8092/8092 [00:00<00:00, 529407.39it/s]\n",
      "100%|██████████| 8092/8092 [00:00<00:00, 529994.35it/s]\n",
      "100%|██████████| 8092/8092 [00:00<00:00, 528163.40it/s]\n",
      "100%|██████████| 8092/8092 [00:00<00:00, 523689.37it/s]\n",
      "100%|██████████| 8092/8092 [00:00<00:00, 507116.72it/s]\n",
      "100%|██████████| 8092/8092 [00:00<00:00, 525221.03it/s]\n",
      "100%|██████████| 8092/8092 [00:00<00:00, 525668.43it/s]\n",
      "100%|██████████| 8092/8092 [00:00<00:00, 525082.89it/s]\n",
      "100%|██████████| 8092/8092 [00:00<00:00, 520078.27it/s]\n",
      "100%|██████████| 8092/8092 [00:00<00:00, 525074.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop complete (demo).\n",
      "dog.jpg: uuhanwwhanvvvvnnhanuu----------------------------------------------------------------------uuii:)uuiiuu~-)||ennyarry--------------------------------------------------------------------------------------------------------------------------------------------^^napjjoooo----------------------------------------------------------------------ennyiiuu\n",
      "horse.jpg: aaaaaaaa''','''������������>>>>>>>>^^^^^^;;����cccc^^ooooaaaa||,,aaaaaaaauu||||^^ffffff;;&&>>>>>>>>^^','','                                                \",\"\n",
      "book.jpg:  أما أما أما“ أما أما أما乔 أما أما أماечно أما أما أما أما أما أما أما أما恒 أما أما恒 أما أما أما外地 sağlam أما\n",
      "bird.jpg: |||| jj;j;j------------->>>>>>>>;p&&||||;;;;jj XXX^^;;||||+\n",
      "^^;;+\n",
      "}}}||||]|}}}iiaaaaaaaa]|;p;;;;;;;;;;;;;;;;aaaa||||\n",
      "person.jpg: ъъъappendъъъъъъъprint________________________________ъъъ:)ъъъъъъъъъъъъъ\n",
      "house.jpg: aghdehyde(companyagh写作没写下aghputeraghaghubernetesgreSQLgreSQL耐磨agh写ubernetesWAREorry unix(...)\n",
      "没 unix]-->\n",
      "wares写作puteraghagh\n",
      "cat.jpg: \\&& **************************************************************************&& **************************************************************************                         ]] **************************************************************************                                ###\n",
      " **************************************************************************0###\n",
      " **************************************************************************                                                                 **************************************************************************[](    \t                       []( **************************************************************************    ????????    ]],NULL **************************************************************************0\n",
      "tree.jpg: คะeci腔tridge blood herfühl出血腔出血 tế tếmotherether her母침母腔腔etherщитmother腔子宫出血 her母ра母\n",
      "car.jpg: long-ddvvwwuuwwooooooooll-------\n",
      "----------------------dddduuelluuaaaallrelliioooouliauuoooooooonn--------\n",
      "oooooooooooooooooonnuu\n",
      "phone.jpg: }|||||ii||||&&ooooiienny}}};)iiiiienny^^iii,&                                                 }}}}}}ii+aii)||###\n",
      ",j''enny)i����#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download Flickr8k and fine-tune CLIP+Qwen (minimal demo)\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download Flickr8k images\n",
    "if not os.path.exists(\"Flickr8k_Dataset\"):\n",
    "    print(\"Downloading Flickr8k images...\")\n",
    "    url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(\"Flickr8k_Dataset.zip\", \"wb\") as f:\n",
    "        for chunk in tqdm(r.iter_content(chunk_size=8192)):\n",
    "            f.write(chunk)\n",
    "    with zipfile.ZipFile(\"Flickr8k_Dataset.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "\n",
    "# Download Flickr8k captions\n",
    "if not os.path.exists(\"Flickr8k_text\"):\n",
    "    print(\"Downloading Flickr8k captions...\")\n",
    "    url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(\"Flickr8k_text.zip\", \"wb\") as f:\n",
    "        for chunk in tqdm(r.iter_content(chunk_size=8192)):\n",
    "            f.write(chunk)\n",
    "    with zipfile.ZipFile(\"Flickr8k_text.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "\n",
    "# Parse captions\n",
    "captions = {}\n",
    "with open(\"Flickr8k.token.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        img, caption = line.strip().split('\\t')\n",
    "        img = img.split('#')[0]\n",
    "        captions.setdefault(img, []).append(caption)\n",
    "\n",
    "# Minimal training loop (single image-caption pair per step, for demo)\n",
    "from transformers import CLIPModel, CLIPImageProcessor, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_encoder = clip_model.vision_model\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B-Base\", torch_dtype=torch.float16)\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n",
    "\n",
    "projection = nn.Linear(clip_model.visual_projection.out_features, qwen_model.model.embed_tokens.embedding_dim, dtype=torch.float16)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = clip_model.to(device)\n",
    "vision_encoder = vision_encoder.to(device)\n",
    "qwen_model = qwen_model.to(device)\n",
    "projection = projection.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(list(projection.parameters()) + list(qwen_model.parameters()), lr=1e-5)\n",
    "\n",
    "# Training loop (VERY minimal, just for demonstration)\n",
    "image_files = list(captions.keys())  # Use all images for demo\n",
    "for epoch in range(10):  # Increase for real training\n",
    "    for img_file in tqdm(image_files):\n",
    "        img_path = os.path.join(\"Flickr8k_Dataset\", \"Flicker8k_Dataset\", img_file)\n",
    "        if not os.path.exists(img_path): continue\n",
    "        caption = captions[img_file][0]  # Use the first caption\n",
    "\n",
    "        # Image to latent\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_inputs = image_processor(image, return_tensors=\"pt\").to(device)\n",
    "        image_features = vision_encoder(**image_inputs).pooler_output\n",
    "        image_features_proj = clip_model.visual_projection(image_features)\n",
    "        image_latent = projection(image_features_proj.half())\n",
    "\n",
    "        # Caption to tokens\n",
    "        input_ids = qwen_tokenizer(caption, return_tensors=\"pt\", truncation=True, max_length=32).input_ids.to(device)\n",
    "        text_embeds = qwen_model.model.embed_tokens(input_ids)\n",
    "        decoder_input = torch.cat([image_latent.unsqueeze(1), text_embeds], dim=1)\n",
    "\n",
    "        # Shift labels for causal LM\n",
    "        labels = input_ids.clone()\n",
    "        outputs = qwen_model(inputs_embeds=decoder_input, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training loop complete (demo).\")\n",
    "\n",
    "image_folder = \"images\"\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(\".jpg\")]\n",
    "\n",
    "for img_file in image_files:\n",
    "    image = Image.open(os.path.join(image_folder, img_file)).convert(\"RGB\")\n",
    "    image_inputs = image_processor(image, return_tensors=\"pt\").to(device)\n",
    "    image_features = vision_encoder(**image_inputs).pooler_output\n",
    "    image_features_proj = clip_model.visual_projection(image_features)\n",
    "    image_latent = projection(image_features_proj.half())\n",
    "\n",
    "    # Generate text from image latent\n",
    "    attention_mask = torch.ones(1, 1, dtype=torch.long).to(device)\n",
    "    position_ids = torch.zeros(1, 1, dtype=torch.long).to(device)\n",
    "    generated_ids = qwen_model.generate(\n",
    "        inputs_embeds=image_latent.unsqueeze(1),\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=qwen_tokenizer.pad_token_id,\n",
    "        eos_token_id=qwen_tokenizer.eos_token_id,\n",
    "    )\n",
    "    description = qwen_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"{img_file}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 1/1012 [00:02<37:43,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.8220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 2/1012 [00:03<26:48,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 3/1012 [00:03<19:10,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 4/1012 [00:04<17:41,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 5/1012 [00:06<18:34,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   1%|          | 6/1012 [00:07<17:39,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   1%|          | 7/1012 [00:07<15:28,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   1%|          | 8/1012 [00:08<13:28,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   1%|          | 9/1012 [00:09<14:22,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   1%|          | 10/1012 [00:09<12:58,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   1%|          | 11/1012 [00:10<11:57,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   1%|          | 12/1012 [00:11<12:08,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   1%|▏         | 13/1012 [00:11<11:52,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   1%|▏         | 14/1012 [00:12<11:30,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   1%|▏         | 15/1012 [00:13<12:17,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   2%|▏         | 16/1012 [00:14<12:12,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   2%|▏         | 17/1012 [00:14<12:33,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   2%|▏         | 18/1012 [00:15<12:48,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   2%|▏         | 19/1012 [00:16<13:21,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   2%|▏         | 20/1012 [00:17<12:56,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   2%|▏         | 21/1012 [00:17<12:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   2%|▏         | 22/1012 [00:18<11:01,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   2%|▏         | 23/1012 [00:19<12:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   2%|▏         | 24/1012 [00:20<12:04,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   2%|▏         | 25/1012 [00:20<11:30,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   3%|▎         | 26/1012 [00:21<11:02,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   3%|▎         | 27/1012 [00:21<11:03,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   3%|▎         | 28/1012 [00:22<10:56,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   3%|▎         | 29/1012 [00:23<10:57,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   3%|▎         | 30/1012 [00:23<11:05,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   3%|▎         | 31/1012 [00:24<12:30,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   3%|▎         | 32/1012 [00:25<11:48,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   3%|▎         | 33/1012 [00:26<11:23,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   3%|▎         | 34/1012 [00:26<11:06,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   3%|▎         | 35/1012 [00:27<10:39,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   4%|▎         | 36/1012 [00:28<10:22,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   4%|▎         | 37/1012 [00:28<10:35,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   4%|▍         | 38/1012 [00:29<10:50,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   4%|▍         | 39/1012 [00:30<10:44,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   4%|▍         | 40/1012 [00:30<10:21,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   4%|▍         | 41/1012 [00:31<09:58,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   4%|▍         | 42/1012 [00:31<10:07,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   4%|▍         | 43/1012 [00:32<09:36,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   4%|▍         | 44/1012 [00:32<09:31,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   4%|▍         | 45/1012 [00:33<09:11,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   5%|▍         | 46/1012 [00:34<09:09,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   5%|▍         | 47/1012 [00:34<09:35,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   5%|▍         | 48/1012 [00:35<09:50,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   5%|▍         | 49/1012 [00:35<09:47,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   5%|▍         | 50/1012 [00:36<09:38,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   5%|▌         | 51/1012 [00:37<09:42,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   5%|▌         | 52/1012 [00:37<10:01,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   5%|▌         | 53/1012 [00:38<10:05,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   5%|▌         | 54/1012 [00:39<09:51,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   5%|▌         | 55/1012 [00:39<09:45,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   6%|▌         | 56/1012 [00:40<09:50,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   6%|▌         | 57/1012 [00:40<09:43,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   6%|▌         | 58/1012 [00:41<09:15,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   6%|▌         | 59/1012 [00:41<09:12,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   6%|▌         | 60/1012 [00:42<09:52,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   6%|▌         | 61/1012 [00:43<09:31,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   6%|▌         | 62/1012 [00:43<09:07,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   6%|▌         | 63/1012 [00:44<09:25,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   6%|▋         | 64/1012 [00:44<09:03,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   6%|▋         | 65/1012 [00:45<09:26,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   7%|▋         | 66/1012 [00:46<09:47,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   7%|▋         | 67/1012 [00:46<10:02,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   7%|▋         | 68/1012 [00:47<09:54,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   7%|▋         | 69/1012 [00:48<09:42,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   7%|▋         | 70/1012 [00:49<11:46,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   7%|▋         | 71/1012 [00:49<11:38,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   7%|▋         | 72/1012 [00:50<11:20,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, CLIPImageProcessor, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Prepare captions dict\n",
    "captions = {}\n",
    "with open(\"Flickr8k.token.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        img, caption = line.strip().split('\\t')\n",
    "        img = img.split('#')[0]\n",
    "        captions.setdefault(img, []).append(caption)\n",
    "\n",
    "# Prepare list of (img_path, caption)\n",
    "image_dir = \"Flicker8k_Dataset\"\n",
    "pairs = []\n",
    "for img in captions:\n",
    "    img_path = os.path.join(image_dir, img)\n",
    "    if os.path.exists(img_path):\n",
    "        pairs.append((img_path, captions[img][0]))\n",
    "\n",
    "# Model setup\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_encoder = clip_model.vision_model\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B-Base\", torch_dtype=torch.float16)\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n",
    "\n",
    "# Use float32 for projection for stability\n",
    "projection = nn.Linear(clip_model.visual_projection.out_features, qwen_model.model.embed_tokens.embedding_dim, dtype=torch.float32)\n",
    "nn.init.xavier_uniform_(projection.weight)\n",
    "if projection.bias is not None:\n",
    "    nn.init.zeros_(projection.bias)\n",
    "\n",
    "# Device selection\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "clip_model = clip_model.to(device)\n",
    "vision_encoder = vision_encoder.to(device)\n",
    "qwen_model = qwen_model.to(device)\n",
    "projection = projection.to(device)\n",
    "\n",
    "# Freeze Qwen, only train projection\n",
    "for param in qwen_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in projection.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.AdamW(projection.parameters(), lr=1e-7)\n",
    "\n",
    "# Collate function for batching and padding\n",
    "def collate_fn(batch):\n",
    "    img_paths, captions_ = zip(*batch)\n",
    "    images = [Image.open(p).convert(\"RGB\") for p in img_paths]\n",
    "    image_inputs = [image_processor(img, return_tensors=\"pt\") for img in images]\n",
    "    merged_image_inputs = {}\n",
    "    for k in image_inputs[0]:\n",
    "        merged_image_inputs[k] = torch.cat([d[k] for d in image_inputs], dim=0)\n",
    "    input_ids = [qwen_tokenizer(c, return_tensors=\"pt\", truncation=True, max_length=32).input_ids[0] for c in captions_]\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=qwen_tokenizer.pad_token_id)\n",
    "    return merged_image_inputs, input_ids_padded\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(pairs, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for image_inputs_batch, input_ids_batch in tqdm(dataloader, desc=f\"Training epoch {epoch+1}\"):\n",
    "        for k in image_inputs_batch:\n",
    "            image_inputs_batch[k] = image_inputs_batch[k].to(device)\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "\n",
    "        # Skip batches with only padding\n",
    "        if (input_ids_batch != qwen_tokenizer.pad_token_id).sum().item() == 0:\n",
    "            continue\n",
    "\n",
    "        image_features = vision_encoder(**image_inputs_batch).pooler_output\n",
    "        image_features_proj = clip_model.visual_projection(image_features)\n",
    "        image_latent = projection(image_features_proj.float())\n",
    "\n",
    "        text_embeds = qwen_model.model.embed_tokens(input_ids_batch)\n",
    "        decoder_input = torch.cat([image_latent.unsqueeze(1), text_embeds], dim=1)\n",
    "\n",
    "        # Pad labels for image token alignment\n",
    "        labels = input_ids_batch.clone()\n",
    "        labels = torch.cat([\n",
    "            torch.full((labels.shape[0], 1), -100, dtype=labels.dtype, device=labels.device),\n",
    "            labels\n",
    "        ], dim=1)\n",
    "        outputs = qwen_model(inputs_embeds=decoder_input, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(projection.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        tqdm.write(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training loop complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
