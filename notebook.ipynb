{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosh/MLX-Week4-CLIP/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPTokenizer, CLIPImageProcessor, AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_encoder = clip_model.vision_model\n",
    "text_encoder = clip_model.text_model\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\"cat\", \"dog\", \"horse\", \"bird\", \"car\", \"person\", \"tree\", \"house\", \"book\", \"phone\"]\n",
    "classifiers = [f\"a photo of a {item}\" for item in items]\n",
    "\n",
    "text_inputs = tokenizer(classifiers, return_tensors=\"pt\")\n",
    "text_features = text_encoder(**text_inputs).pooler_output\n",
    "text_features_proj = clip_model.text_projection(text_features)\n",
    "\n",
    "for item, classifier in zip(items, classifiers):\n",
    "    image = Image.open(f\"images/{item}.jpg\")\n",
    "    image_inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    image_features = vision_encoder(**image_inputs).pooler_output\n",
    "    image_features_proj = clip_model.visual_projection(image_features)\n",
    "    \n",
    "    # Compute similarity to all classifier texts\n",
    "    similarities = (image_features_proj @ text_features_proj.T).squeeze(0)\n",
    "    best_idx = similarities.argmax().item()\n",
    "    best_label = classifiers[best_idx]\n",
    "    best_score = similarities[best_idx].item()\n",
    "    \n",
    "    print(f\"{item}: best match is '{best_label}' (score: {best_score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'items' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m qwen_dim = qwen_model.model.embed_tokens.embedding_dim  \u001b[38;5;66;03m# 4096\u001b[39;00m\n\u001b[32m      4\u001b[39m adapter = nn.Sequential(\n\u001b[32m      5\u001b[39m     nn.Linear(clip_dim, qwen_dim),\n\u001b[32m      6\u001b[39m     nn.LayerNorm(qwen_dim),\n\u001b[32m      7\u001b[39m     nn.GELU(),\n\u001b[32m      8\u001b[39m     nn.Linear(qwen_dim, qwen_dim),\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mitems\u001b[49m:\n\u001b[32m     12\u001b[39m     image = Image.open(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimages/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jpg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m     image_inputs = image_processor(image, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'items' is not defined"
     ]
    }
   ],
   "source": [
    "clip_dim = clip_model.visual_projection.out_features  # 512\n",
    "qwen_dim = qwen_model.model.embed_tokens.embedding_dim  # 4096\n",
    "\n",
    "adapter = nn.Sequential(\n",
    "    nn.Linear(clip_dim, qwen_dim),\n",
    "    nn.LayerNorm(qwen_dim),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(qwen_dim, qwen_dim),\n",
    ")\n",
    "\n",
    "for item in items:\n",
    "    image = Image.open(f\"images/{item}.jpg\")\n",
    "    image_inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    image_features = vision_encoder(**image_inputs).pooler_output\n",
    "    image_features_proj = clip_model.visual_projection(image_features)\n",
    "    image_latent = adapter(image_features_proj)\n",
    "    attention_mask = torch.ones(1, 1)\n",
    "    position_ids = torch.zeros(1, 1)\n",
    "\n",
    "    generated_ids = qwen_model.generate(inputs_embeds=image_latent.unsqueeze(1), attention_mask=attention_mask, position_ids=position_ids, max_new_tokens=30, do_sample=True, temperature=0.8, pad_token_id=qwen_tokenizer.pad_token_id, eos_token_id=qwen_tokenizer.eos_token_id)\n",
    "\n",
    "    print(f\"{item}: {qwen_tokenizer.decode(generated_ids[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Flickr30k dataset...\n",
      "Dataset loaded: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "        num_rows: 31014\n",
      "    })\n",
      "})\n",
      "Available splits: ['test']\n",
      "Test set: 31014 samples\n",
      "Loaded 31014 images with captions\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Load Flickr30k dataset from Hugging Face\n",
    "print(\"Loading Flickr30k dataset...\")\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "\n",
    "\n",
    "print(f\"Dataset loaded: {dataset}\")\n",
    "print(f\"Available splits: {list(dataset.keys())}\")\n",
    "print(f\"Test set: {len(dataset['test'])} samples\")\n",
    "\n",
    "# Convert to the format your training code expects\n",
    "captions = {}\n",
    "for item in dataset['test']:  # Changed from 'train' to 'test'\n",
    "    img_filename = item['filename']  # Changed from 'image_file_name' to 'filename'\n",
    "    caption = item['caption']  # Changed from 'sentence' to 'caption'\n",
    "    captions[img_filename] = caption  # Don't use setdefault with append\n",
    "    \n",
    "print(f\"Loaded {len(captions)} images with captions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model = clip_model.to(device)\n",
    "vision_encoder = vision_encoder.to(device)\n",
    "qwen_model = qwen_model.to(device)\n",
    "adapter = adapter.to(device)\n",
    "\n",
    "\n",
    "# dont change qwen weights\n",
    "for param in qwen_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(adapter.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "sample_size = 10  # Instead of 31,014 samples\n",
    "dataset = dataset['test'].select(range(sample_size))\n",
    "\n",
    "image_files = list(captions.keys())  # Use all images for demo\n",
    "for epoch in range(3):\n",
    "    for item in tqdm(dataset):\n",
    "        # Get image and caption\n",
    "        image = item['image']\n",
    "        caption = item['caption']\n",
    "        \n",
    "        # Process image\n",
    "        image_inputs = image_processor(image, return_tensors=\"pt\").to(device)\n",
    "        image_features = vision_encoder(**image_inputs).pooler_output\n",
    "        image_features_proj = clip_model.visual_projection(image_features)\n",
    "        image_latent = adapter(image_features_proj)\n",
    "\n",
    "        # Tokenize the caption\n",
    "        input_ids = qwen_tokenizer(caption, return_tensors=\"pt\", truncation=True, max_length=32).input_ids.to(device)\n",
    "        \n",
    "        # Create the full sequence: [image_embedding] + [caption_tokens]\n",
    "        image_latent_seq = image_latent.unsqueeze(1)  # [1, 1, hidden_dim]\n",
    "        \n",
    "        # Get text embeddings for the caption\n",
    "        text_embeds = qwen_model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        # Concatenate: image embedding + text embeddings\n",
    "        full_embeddings = torch.cat([image_latent_seq, text_embeds], dim=1)\n",
    "        \n",
    "        # Create labels: -100 for image position, actual tokens for text\n",
    "        batch_size, seq_len = full_embeddings.size(0), full_embeddings.size(1)\n",
    "        labels = torch.full((batch_size, seq_len), -100, dtype=torch.long, device=device)\n",
    "        labels[:, 1:1+input_ids.size(1)] = input_ids  # Fill text positions with actual tokens\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = qwen_model(inputs_embeds=full_embeddings, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training loop complete (demo).\")\n",
    "\n",
    "# Save the trained models\n",
    "print(\"Saving trained models...\")\n",
    "torch.save(adapter.state_dict(), \"adapter.pth\")\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the adapter on your local images with simple captions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cpu\"  # Use CPU for training\n",
    "\n",
    "# Move models to device\n",
    "clip_model = clip_model.to(device)\n",
    "vision_encoder = vision_encoder.to(device)\n",
    "qwen_model = qwen_model.to(device)\n",
    "adapter = adapter.to(device)\n",
    "\n",
    "# Freeze Qwen weights\n",
    "for param in qwen_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(adapter.parameters(), lr=1e-4)  # Higher learning rate\n",
    "\n",
    "# Simple training data - your local images\n",
    "training_data = [\n",
    "    (\"cat.jpg\", \"a cat\"),\n",
    "    (\"dog.jpg\", \"a dog\"), \n",
    "    (\"horse.jpg\", \"a horse\"),\n",
    "    (\"bird.jpg\", \"a bird\"),\n",
    "    (\"car.jpg\", \"a car\"),\n",
    "    (\"person.jpg\", \"a person\"),\n",
    "    (\"tree.jpg\", \"a tree\"),\n",
    "    (\"house.jpg\", \"a house\"),\n",
    "    (\"book.jpg\", \"a book\"),\n",
    "    (\"phone.jpg\", \"a phone\")\n",
    "]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# Train for a few epochs\n",
    "for epoch in range(20):  # More epochs\n",
    "    total_loss = 0\n",
    "    for image_name, caption in training_data:\n",
    "        # Load image\n",
    "        image = Image.open(f\"images/{image_name}\")\n",
    "        image_inputs = image_processor(image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Get CLIP features\n",
    "        image_features = vision_encoder(**image_inputs).pooler_output\n",
    "        image_features_proj = clip_model.visual_projection(image_features)\n",
    "        image_latent = adapter(image_features_proj)\n",
    "\n",
    "        # Tokenize caption\n",
    "        input_ids = qwen_tokenizer(caption, return_tensors=\"pt\", truncation=True, max_length=10).input_ids.to(device)\n",
    "        \n",
    "        # Create sequence: [image_embedding] + [caption_tokens]\n",
    "        image_latent_seq = image_latent.unsqueeze(1)\n",
    "        text_embeds = qwen_model.model.embed_tokens(input_ids)\n",
    "        full_embeddings = torch.cat([image_latent_seq, text_embeds], dim=1)\n",
    "        \n",
    "        # Create labels\n",
    "        batch_size, seq_len = full_embeddings.size(0), full_embeddings.size(1)\n",
    "        labels = torch.full((batch_size, seq_len), -100, dtype=torch.long, device=device)\n",
    "        labels[:, 1:1+input_ids.size(1)] = input_ids\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = qwen_model(inputs_embeds=full_embeddings, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss/len(training_data):.4f}\")\n",
    "\n",
    "# Save the trained adapter\n",
    "torch.save(adapter.state_dict(), \"adapter_trained.pth\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "adapter.load_state_dict(torch.load(\"adapter.pth\"))\n",
    "adapter = adapter.float()\n",
    "adapter = adapter.to(device)\n",
    "adapter.eval()  # Set to evaluation mode\n",
    "\n",
    "clip_model = clip_model.to(device)\n",
    "vision_encoder = vision_encoder.to(device)\n",
    "qwen_model = qwen_model.to(device)\n",
    "qwen_model = qwen_model.float()\n",
    "qwen_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Test generation\n",
    "for item in items:\n",
    "    image = Image.open(f\"images/{item}.jpg\")\n",
    "    image_inputs = image_processor(image, return_tensors=\"pt\").to(device)\n",
    "    image_features = vision_encoder(**image_inputs).pooler_output\n",
    "    image_features_proj = clip_model.visual_projection(image_features)\n",
    "    image_latent = adapter(image_features_proj)\n",
    "    \n",
    "    image_latent = image_latent.float()\n",
    "    attention_mask = torch.ones(1, 1, device=device, dtype=torch.long)\n",
    "    position_ids = torch.zeros(1, 1, device=device, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # FAST generation - no sampling, fewer tokens\n",
    "        generated_ids = qwen_model.generate(\n",
    "            inputs_embeds=image_latent.unsqueeze(1), \n",
    "            attention_mask=attention_mask, \n",
    "            position_ids=position_ids, \n",
    "            max_new_tokens=10,  # Reduced from 30\n",
    "            do_sample=False,    # Greedy decoding (much faster)\n",
    "            # temperature=0.8,  # Remove this\n",
    "            pad_token_id=qwen_tokenizer.pad_token_id, \n",
    "            eos_token_id=qwen_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    print(f\"{item}: {qwen_tokenizer.decode(generated_ids[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation WITHOUT training first to see if it's fast\n",
    "import os\n",
    "\n",
    "device = \"cpu\"  # Use CPU for now - it's often faster than MPS for small models\n",
    "\n",
    "\n",
    "adapter.load_state_dict(torch.load(\"adapter.pth\"))\n",
    "adapter = adapter.float()\n",
    "adapter = adapter.to(device)\n",
    "adapter.eval()  # Set to evaluation mode\n",
    "\n",
    "clip_model = clip_model.to(device)\n",
    "vision_encoder = vision_encoder.to(device)\n",
    "qwen_model = qwen_model.to(device)\n",
    "qwen_model = qwen_model.float()\n",
    "qwen_model.eval()  # Set to evaluation mode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test generation with the small model\n",
    "for item in items:\n",
    "    image = Image.open(f\"images/{item}.jpg\")\n",
    "    image_inputs = image_processor(image, return_tensors=\"pt\").to(device)\n",
    "    image_features = vision_encoder(**image_inputs).pooler_output\n",
    "    image_features_proj = clip_model.visual_projection(image_features)\n",
    "    image_latent = adapter(image_features_proj)\n",
    "    \n",
    "    attention_mask = torch.ones(1, 1, device=device, dtype=torch.long)\n",
    "    position_ids = torch.zeros(1, 1, device=device, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = qwen_model.generate(\n",
    "            inputs_embeds=image_latent.unsqueeze(1), \n",
    "            attention_mask=attention_mask, \n",
    "            position_ids=position_ids, \n",
    "            max_new_tokens=5,  # Very short\n",
    "            do_sample=False,   # Greedy\n",
    "            pad_token_id=qwen_tokenizer.pad_token_id, \n",
    "            eos_token_id=qwen_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    print(f\"{item}: {qwen_tokenizer.decode(generated_ids[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:  Edoardo, the only son of the last Italian-born governor of the island of Tuscany, Italy, with his wife, Lucrezia\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# --- Device setup ---\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# --- Move models to device ---\n",
    "clip_model = clip_model.to(device)\n",
    "vision_encoder = vision_encoder.to(device)\n",
    "adapter = adapter.to(device)\n",
    "qwen_model = qwen_model.to(device)\n",
    "\n",
    "clip_model.eval()\n",
    "vision_encoder.eval()\n",
    "adapter.eval()\n",
    "qwen_model.eval()\n",
    "\n",
    "adapter.load_state_dict(torch.load(\"adapter_flickr_batched.pth\"))\n",
    "\n",
    "\n",
    "# --- Inputs ---\n",
    "image = Image.open(\"images/cat.jpg\")\n",
    "prompt = \"This is a photo of:\"  # You can change this prompt\n",
    "\n",
    "# --- 1. Image to CLIP features ---\n",
    "image_inputs = image_processor(image, return_tensors=\"pt\")\n",
    "image_inputs = {k: v.to(device) for k, v in image_inputs.items()}\n",
    "with torch.no_grad():\n",
    "    image_features = vision_encoder(**image_inputs).pooler_output  # [1, 1, vision_dim]\n",
    "    image_features_proj = clip_model.visual_projection(image_features)  # [1, 1, clip_dim]\n",
    "    image_latent = adapter(image_features_proj)  # [1, 1, qwen_dim]\n",
    "    image_latent = image_latent.unsqueeze(1)  # [1, 1, qwen_dim]\n",
    "\n",
    "# --- 2. Prompt to Qwen embeddings ---\n",
    "input_ids = qwen_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)  # [1, prompt_len]\n",
    "with torch.no_grad():\n",
    "    text_embeds = qwen_model.model.embed_tokens(input_ids)  # [1, prompt_len, qwen_dim]\n",
    "\n",
    "# --- 3. Concatenate image and text embeddings ---\n",
    "full_embeddings = torch.cat([image_latent, text_embeds], dim=1)  # [1, 1+prompt_len, qwen_dim]\n",
    "\n",
    "# --- 4. Attention mask ---\n",
    "batch_size, seq_len, _ = full_embeddings.shape\n",
    "attention_mask = torch.ones((batch_size, seq_len), dtype=torch.long, device=device)\n",
    "\n",
    "# --- 5. Generate text ---\n",
    "with torch.no_grad():\n",
    "    generated_ids = qwen_model.generate(\n",
    "        inputs_embeds=full_embeddings,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=30,\n",
    "        pad_token_id=qwen_tokenizer.pad_token_id,\n",
    "        eos_token_id=qwen_tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "    )\n",
    "    output = qwen_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"Generated:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batched adapter training on Flickr30k...\n",
      "Batch 1/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [05:53<00:00, 27.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 6.1941\n",
      "Batch 2/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [08:02<00:00, 37.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 6.1678\n",
      "Batch 3/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [08:33<00:00, 39.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 5.8785\n",
      "Batched adapter training complete and saved as adapter_flickr_batched.pth!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Move models to device\n",
    "clip_model = clip_model.to(device)\n",
    "vision_encoder = vision_encoder.to(device)\n",
    "qwen_model = qwen_model.to(device)\n",
    "adapter = adapter.to(device)\n",
    "\n",
    "clip_model.eval()\n",
    "vision_encoder.eval()\n",
    "qwen_model.eval()\n",
    "adapter.train()\n",
    "\n",
    "# Freeze CLIP and Qwen\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in vision_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in qwen_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(adapter.parameters(), lr=1e-4)\n",
    "\n",
    "# Prepare a PyTorch dataset and dataloader\n",
    "class FlickrDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, image_processor):\n",
    "        self.data = hf_dataset\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = item['image']\n",
    "        caption = item['caption']\n",
    "        if isinstance(caption, list):\n",
    "            caption = caption[0]\n",
    "        return image, caption\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    # Process images as a batch (no padding argument!)\n",
    "    image_inputs = image_processor(list(images), return_tensors=\"pt\")\n",
    "    # Tokenize captions as a batch (padding is correct here)\n",
    "    tokenized = qwen_tokenizer(\n",
    "        list(captions),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=32\n",
    "    )\n",
    "    return image_inputs, tokenized\n",
    "\n",
    "batch_size = 8\n",
    "sample_size = 100  # Increase for more data\n",
    "flickr_data = dataset['test'].select(range(sample_size))\n",
    "flickr_dataset = FlickrDataset(flickr_data, image_processor)\n",
    "dataloader = DataLoader(flickr_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Starting batched adapter training on Flickr30k...\")\n",
    "for epoch in range(3):\n",
    "    print(f\"Batch {epoch+1}/{len(dataloader)}\")\n",
    "    total_loss = 0\n",
    "    for image_inputs, tokenized in tqdm(dataloader):\n",
    "        # Move to device\n",
    "        image_inputs = {k: v.to(device) for k, v in image_inputs.items()}\n",
    "        input_ids = tokenized['input_ids'].to(device)\n",
    "        attention_mask = tokenized['attention_mask'].to(device)\n",
    "\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = vision_encoder(**image_inputs).pooler_output  # [B, vision_dim]\n",
    "            image_features_proj = clip_model.visual_projection(image_features)  # [B, clip_dim]\n",
    "        image_latent = adapter(image_features_proj)  # [B, qwen_dim]\n",
    "        image_latent = image_latent.unsqueeze(1)  # [B, 1, qwen_dim]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_embeds = qwen_model.model.embed_tokens(input_ids)  # [B, seq_len, qwen_dim]\n",
    "\n",
    "        # Concatenate image and text embeddings\n",
    "        full_embeddings = torch.cat([image_latent, text_embeds], dim=1)  # [B, 1+seq_len, qwen_dim]\n",
    "\n",
    "        # Labels: -100 for image, actual tokens for text\n",
    "        labels = torch.full((batch_size, seq_len + 1), -100, dtype=torch.long, device=device)\n",
    "        labels[:, 1:] = input_ids\n",
    "\n",
    "        # Forward and optimize\n",
    "        outputs = qwen_model(inputs_embeds=full_embeddings, labels=labels, attention_mask=torch.cat([torch.ones((batch_size, 1), device=device, dtype=torch.long), attention_mask], dim=1))\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss/len(flickr_dataset):.4f}\")\n",
    "\n",
    "torch.save(adapter.state_dict(), \"adapter_flickr_batched.pth\")\n",
    "print(\"Batched adapter training complete and saved as adapter_flickr_batched.pth!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
